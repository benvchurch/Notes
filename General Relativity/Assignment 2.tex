\documentclass[12pt]{article}
\usepackage[english]{babel}
\include{GRCommands}
\begin{document}

\atitle{2}

\section*{1.}

Let $\tilde{\epsilon}^{\alpha \beta \gamma \delta}$ be the totally antisymmetric symbol in $d = 1 + 3$ dimensional Minkowski space. Define, 
\[ \epsilon^{\alpha \beta \gamma \delta} = \frac{1}{\sqrt{-g}} \tilde{\epsilon}^{\alpha \beta \gamma \delta} \]
 Consider a coordinate transformation $\Lambda^\mu_{\mu'}$ which preserves handedness i.e. $\det{\Lambda} = \det{\Lambda^{\mu}_{\mu'}} > 0$. 
Using the well-known cofactor expansion formula,
\[ \Lambda^{\alpha'}_{\alpha} \Lambda^{\beta'}_{\beta} \Lambda^{\gamma'}_{\gamma} \Lambda^{\delta'}_{\delta} \epsilon^{\alpha \beta \gamma \delta} = \Lambda^{\alpha'}_{\alpha} \Lambda^{\beta'}_{\beta} \Lambda^{\gamma'}_{\gamma} \Lambda^{\delta'}_{\delta} \frac{1}{\sqrt{-g}} \tilde{\epsilon}^{\alpha \beta \gamma \delta} = \frac{\det{\Lambda^{-1}}}{\sqrt{- g}}  \: \tilde{\epsilon}^{\alpha' \beta' \gamma' \delta'}   \]
However, 
\[ g' = \det{g'_{\alpha' \beta'}} = \det{(\Lambda^\alpha_{\alpha'} \Lambda^{\beta}_{\beta'})} = (\det{\Lambda})^2 \det{g_{\alpha \beta}} = (\det{\Lambda})^2 g \]
Therefore, since $\det{\Lambda} > 0$ we have,
\[ \sqrt{-g'} = \sqrt{-g} \det{\Lambda} \] 
and thus,
\[ \Lambda^{\alpha'}_{\alpha} \Lambda^{\beta'}_{\beta} \Lambda^{\gamma'}_{\gamma} \Lambda^{\delta'}_{\delta} \epsilon^{\alpha \beta \gamma \delta} = \frac{1}{\sqrt{-g'}} \tilde{\epsilon}^{\alpha' \beta' \gamma' \delta'} = \epsilon^{\alpha' \beta' \gamma' \delta'} \]
proving that $\epsilon^{\alpha \beta \gamma \delta}$ transforms as a $(4, 0)$-tensor.

\section*{2.}

Let $a^{ij}$ be a $(2,0)$-tensor in Euclidean $\R^3$. Let $\vec{e}_i$ be the coordinate basis vectors for the given (unprimed) coordinates. Then, $a^{ij} \vec{e}_i \otimes \vec{e}_j$ is an invariant object. Therefore, 
\[ \pderiv{}{x^k} (a^{ij} \vec{e}_i \otimes \vec{e}_j) = (\partial_k a^{ij})\vec{e}_i \otimes \vec{e}_j + a^{ij} (\partial_k \vec{e}_i) \otimes \vec{e}_j + a^{ij} \vec{e}_i \otimes (\partial_k \vec{e}_j) \]
must transform tensorially with rank $(0, 1)$ since it is the derivative of an invariant. However, expressing these quantities in terms of Christoffel symbols we find,
\begin{align*}
\pderiv{}{x^k} (a^{ij} \vec{e}_i \otimes \vec{e}_j) & = (\partial_k a^{ij})\vec{e}_i \otimes \vec{e}_j + a^{ij} \Gamma^m_{ki} \vec{e}_m \otimes \vec{e}_j + a^{ij} \Gamma^m_{kj} \vec{e}_i \otimes \vec{e}_m
\\
& = (\partial_k a^{ij})\vec{e}_i \otimes \vec{e}_j + a^{lj} \Gamma^i_{kl} \vec{e}_i \otimes \vec{e}_j + a^{il} \Gamma^j_{kl} \vec{e}_i \otimes \vec{e}_j
\\
& = \left (\partial_k a^{ij}  + a^{lj} \Gamma^i_{kl}  + a^{il} \Gamma^j_{kl} \right) \vec{e}_i \otimes \vec{e}_j
\end{align*}
Therefore, since $\vec{e}_i \otimes \vec{e}_j$ transforms tensorially with rank $(0,2)$ and the contraction is an invariant, we must have that
\[ \nabla_k a^{ij} = \partial_k a^{ij}  + a^{lj} \Gamma^i_{kl}  + a^{il} \Gamma^j_{kl} \]
is a tensor of rank $(2, 1)$. Furthermore, in Cartesian coordinates, $\Gamma^{k}_{ij} = 0$ identically so,
\[ \nabla_k a^{ij} = \partial_k a^{ij} \]
Any tensor which extends $\partial_k a^{ij}$ must then agree with this expression for $\nabla_k a^{ij}$ in Cartesian coordinates and thus by the tensor property must agree in all coordinate systems.

\newcommand{\ihat}{\boldsymbol{\hat{\imath}}}
\newcommand{\jhat}{\boldsymbol{\hat{\jmath}}} 
\newcommand{\khat}{\boldsymbol{\hat{k}}}

\section*{3.}

Consider Euclidean space $E^3$ with shperical coordinates $(r, \theta, \phi)$. We can parametrize Cartesian coordinates via,
\[ (x,y,z) = (r \sin{\theta} \cos{\phi}, r \sin{\theta} \sin{\phi}, r \cos{\theta}) \]
Therefore, we can find the unit vectors via,
\[ \vec{e}_i = \pderiv{\vec{R}}{q^i} = \pderiv{}{q^i} \left( x \ihat + y \jhat + z \khat \right) \]
In terms of spherical coordinates these give,
\begin{align*}
\vec{e}_r & = \pderiv{}{r} \left( x \ihat + y \jhat + z \khat \right) = \ihat \sin{\theta} \cos{\phi} + \jhat \sin{\theta} \sin{\phi} + \khat \cos{\theta}  
\\
\vec{e}_\theta & = \pderiv{}{\theta} \left( x \ihat + y \jhat + z \khat \right) = \ihat r \cos{\theta} \cos{\phi} + \jhat r \cos{\theta} \sin{\phi} - \khat r \sin{\theta} 
\\
\vec{e}_\phi & = \pderiv{}{\phi} \left( x \ihat + y \jhat + z \khat \right) = - \ihat r \sin{\theta} \sin{\phi} + \jhat r \sin{\theta} \cos{\phi} 
\end{align*}
From these we may compute the metric,
\[ g_{ij} = \vec{e}_i \cdot \vec{e}_j = 
\begin{pmatrix}
1 & 0 & 0
\\
0 & r^2 & 0 
\\
0 & 0 & r^2 \sin^2{\theta} 
\end{pmatrix} 
\quad \text{and} \quad g^{ij} = 
\begin{pmatrix}
1 & 0 & 0
\\
0 & \frac{1}{r^2} & 0 
\\
0 & 0 & \frac{1}{r^2 \sin^{2}{\theta}}
\end{pmatrix}  \]
Then the Christoffel symbols may be computed from the formula,
\[ \Gamma^k_{ij} = \tfrac{1}{2} g^{km} \left( \partial_i g_{mj} + \partial_j g_{im} - \partial_m g_{ij} \right) \]
Plugging in, I find,
\begin{align*}
\Gamma^r_{ij} & =
\begin{pmatrix}
0 & 0 & 0
\\
0 & -r & 0
\\
0 & 0 & -r \sin^{2}{\theta}
\end{pmatrix}
\quad \quad 
\Gamma^\theta_{ij} =
\begin{pmatrix}
0 & r^{-1} & 0
\\
r^{-1} & 0 & 0
\\
0 & 0 & - \cos{\theta} \sin{\theta}
\end{pmatrix}
\quad \quad 
\Gamma^\phi_{ij} =
\begin{pmatrix}
0 & 0 & r^{-1}
\\
0 & 0 & \cot{\theta}
\\
r^{-1} & \cot{\theta} & 0
\end{pmatrix}
\end{align*}

\section*{4.}

Consider the electromagnetic field tensor $F_{\mu \nu} = \partial_\mu A_\nu - \partial_\nu A_\mu$ which satisfies,
\[ \partial_\mu F^{\mu \nu} = \frac{4 \pi}{c} j^\nu \quad \text{and} \quad \partial_{\mu} * F^{\mu \nu} = 0 \]
The electromagnetic tensor takes on the explict form,
\begin{align*}
F_{\mu \nu} =
\begin{pmatrix}
0 & - E_x & - E_y & - E_z
\\
E_x & 0 & B_z & - B_y
\\
E_y & - B_z & 0 & B_x
\\
E_z & B_y & - B_x & 0
\end{pmatrix}
\end{align*}
Consider the transformation associated to a rotation about the $y$-axis,
\[ \Lambda = 
\begin{pmatrix}
1 & 0 & 0 & 0
\\
0 & \cos{\theta} &  0 & \sin{\theta}
\\
0 & 0 & 1 & 0 
\\
0 & - \sin{\theta} & 0 & \cos{\theta}
\end{pmatrix} \]
Then the 2-form $F$ as a matrix transforms as,
\[ F' = \Lambda F \Lambda^\top = 
\begin{pmatrix}
0 & - E_x \cos{\theta} - E_z \sin{\theta} & - E_y & - E_z \cos{\theta} + E_x \sin{\theta}
\\
E_x \cos{\theta} + E_z \sin{\theta} & 0 & B_z \cos{\theta} - B_x \sin{\theta} & - B_y
\\
E_y  & - B_z & 0 & B_x \cos{\theta} + B_z \sin{\theta}
\\
E_z \cos{\theta} - E_x \sin{\theta} & B_y  & - B_x \cos{\theta} - B_z \sin{\theta} & 0
\end{pmatrix} \]
Therefore the fields $\bf{E}$ and $\bf{B}$ transform as vectors under rotation,
\begin{align*}
E_x & \mapsto E_x \cos{\theta} + E_z \sin{\theta} \quad E_y \mapsto E_y \quad E_z \mapsto E_z \cos{\theta} - E_x \sin{\theta}
\\
B_x & \mapsto B_x \cos{\theta} + B_z \sin{\theta} \quad B_y \mapsto B_y \quad B_z \mapsto B_z \cos{\theta} - B_x \sin{\theta}
\end{align*}
Now consider the transformation associated to a boost along the $z$-axis,
\[ \Lambda = 
\begin{pmatrix}
\cosh{\eta} & 0 & 0 & \sinh{\eta}
\\
0 & 1 &  0 & 0
\\
0 & 0 & 1 & 0 
\\
\sinh{\eta} & 0 & 0 & \cosh{\eta}
\end{pmatrix} \]
Then the 2-form $F$ as a matrix transforms as,
\begin{align*}
F' & = \Lambda F \Lambda^\top 
\\
& = 
\begin{pmatrix}
0 & - E_x \cosh{\eta} + B_y \sinh{\eta} & - E_y \cosh{\eta} - B_x \sinh{\eta} & - E_z
\\
E_x \cos{\eta} - B_y \sinh{\eta} & 0 & B_z & - B_y \cosh{\eta} + E_x \sinh{\eta}
\\
E_y \cosh{\eta} + B_x \sinh{\eta} & - B_z & 0 & B_x \cosh{\eta} + E_y \sinh{\eta}
\\
E_z  & B_y \cosh{\eta} - E_x \sinh{\eta} & - B_x \cosh{\eta} - E_y \sinh{\eta} & 0
\end{pmatrix}
\end{align*}
Therefore the fields $\bf{E}$ and $\bf{B}$ transform under $z$-boosts by,
\begin{align*}
E_x & \mapsto E_x \cosh{\eta} - B_y \sinh{\eta} \quad E_y \mapsto E_y \cosh{\eta} + B_x \sinh{\eta} \quad E_z \mapsto E_z 
\\
B_x & \mapsto B_x \cosh{\eta} + E_y \sinh{\eta} \quad B_y \mapsto B_y \cosh{\eta} - E_x \sinh{\eta} \quad B_z \mapsto B_z
\end{align*}
\section*{5.}

The electromagnetic field tensor $F_{\mu \nu} = \partial_\mu A_\nu - \partial_\nu A_\mu$ satisfies,
\[ \partial_\mu F^{\mu \nu} = \frac{4 \pi}{c} j^\nu \quad \text{and} \quad \partial_{\mu} * F^{\mu \nu} = 0 \]
In components, 
\begin{align*}
F_{\mu \nu} =
\begin{pmatrix}
0 & - E_x & - E_y & - E_z
\\
E_x & 0 & B_z & - B_y
\\
E_y & - B_z & 0 & B_x
\\
E_z & B_y & - B_x & 0
\end{pmatrix}
\quad \quad \quad
* F^{\mu \nu} =
\begin{pmatrix}
0 & B_x & B_y & B_z
\\
- B_x & 0 & -E_z & E_y
\\
- B_y & E_z & 0 & -E_x
\\
- B_z & -E_y & E_x & 0
\end{pmatrix}
\end{align*}
Furthermore, the charge current four vector has components,
\[ j^\nu = (c \rho, \bf{j}) \]
Therefore, the first equation becomes,
\[ \nabla \cdot \bf{E} = 4 \pi \rho \quad \quad - \pderiv{}{t} \bf{E} + \nabla \times \bf{B} = \frac{4 \pi}{c} \bf{J} \]
And the second becomes,
\[ \nabla \cdot \bf{B} = 0 \quad \quad \quad \pderiv{}{t} \bf{B} + \nabla \times \bf{E} = 0 \]


\section*{6.}

\renewcommand{\d}[1]{\mathrm{d} #1}

\subsection*{(a)}

The fact that any exact form is closed is equivalent to the condition $\d{\d{\omega}} = 0$ for any $p$-form $\omega$. Consider,
\begin{align*}
(\d{\d{\omega}})_{\mu_1 \dots \mu_{p+2}} = (p + 2)(p + 1) \partial_{[\mu_1 } \partial_{[\mu_2} \omega_{\mu_3 \dots \mu_{p + 2}]]} 
\end{align*}
Now, each term $\partial_{[\mu_i} \partial_{\mu_j} \omega_{\sigma_1 \dots \sigma_p]}$ is symmetric in $i$ and $j$ since partial derivatives commute. However, the entire form is antisymmetric in all variables so each of these terms vanishes. Thus, $\d{\d{\omega}} = 0$. 

\subsection*{(b)}

Let $\omega$ be a $p$-form. Consider,
\begin{align*}
\nabla_{[\mu_1} \omega_{\mu_2 \dots \mu_{p+1}]} = \partial_{[ \mu_1} \omega_{\mu_2 \dots \mu_{p+1}]} + \Gamma^\nu_{[\mu_1 \mu_2} \omega_{\nu \dots \mu_{p+1}]} + \cdots + \Gamma^{\nu}_{[\mu_1 \mu_{p+1}} \omega_{\mu_2 \dots \nu]}
\end{align*}
However $\Gamma^{\alpha}_{\mu \nu}$ is symmetric in $\mu \iff \nu$ and therefore each additional term must give zero when anti-symmetrized over indices including both lower indices of $\Gamma$. Therefore,
\[ \nabla_{[\mu_1} \omega_{\mu_2 \dots \mu_{p+1}]} = \partial_{[ \mu_1} \omega_{\mu_2 \dots \mu_{p+1}]} = (\d{\omega})_{\mu_1 \dots \mu_{p + 1}} \]

\section*{7.}


Let $\omega$ be a $p$-form and $\eta$ be a $q$-form. 
We need to show that $\d{(\omega \wedge \eta)} = \d{\omega} \wedge \eta + (-1)^{p} \omega \wedge \d{\eta}$. I will prove this claim by induction on $q$. First, suppose that $q = 0$ then $\eta = f$ some smooth function and thus,
\[ \d{(\omega \wedge \eta)} = \d{(f \omega)})_{\mu_1 \dots \mu_{p+1}]} = (p + 1) \partial_{[ \mu_1} (f \eta_{\mu_2 \dots \mu_{p+1}]}) = (p + 1) (\partial_{[ \mu_1} \eta_{\mu_2 \dots \mu_{p+1}]}) f + (p + 1) (\partial_{[ \mu_1} f) \eta_{\mu_2 \dots \mu_{p+1}]} \]
Now I can swap the order of the indices in the last term by introducing $p$ swaps and thus $p$ sign flips,
\[ \d{(\omega \wedge \eta)} =  (p + 1) (\partial_{[ \mu_1} \eta_{\mu_2 \dots \mu_{p+1}]}) f + (p + 1) (-1)^{p}  \eta_{[\mu_2 \dots \mu_{p+1}} (\partial_{ \mu_1]} f) = \d{\omega} \wedge f + (-1)^p \omega \wedge \d{f} \]
Where the factor $(p + 1) = \frac{(p + 1)!}{p! 1!}$ is absorbed into the definition of $\omega \wedge \d{f}$. Now we assume the induction hypothesis that $\d{(\omega \wedge \eta)} = \d{\omega} \wedge \eta + (-1)^{p} \omega \wedge \d{\eta}$ for forms of any $p$ and fixed $q$ which we are inducting on. Now consider the $p+1$-form $\eta \wedge \beta$ where $\beta$ is a $1$-form. Clearly this is not a general $p+1$-form however we have shown that the above formula respects linear combination and scaling by smooth functions. Therefore, it suffices to prove the induction step for $p+1$-forms which can be writen as $\eta \wedge \beta$ since any $p+1$-form can then be built from linear combinations and scaling by smooth functions. Therefore, I must show,
\[ \d{(\omega \wedge \eta \wedge \beta)} = \d{(\omega \wedge \eta)} \wedge \beta + (-1)^{p + q} (\omega \wedge \eta) \wedge \d{\beta} \]
for $1$-forms. Given this,
\begin{align*}
\d{(\omega \wedge \eta \wedge \beta)} & = \d{\omega} \wedge \eta \wedge \beta + (-1)^p \omega \wedge \d{\eta} \wedge \beta + (-1)^{p + q} \omega \wedge \eta \wedge \beta 
\\
& = \d{\omega} \wedge (\eta \wedge \beta) + (-1)^p \omega \wedge [\d{\eta} \wedge \beta + (-1)^q \eta \wedge \d{\beta}]
\\
& = \d{\omega} \wedge (\eta \wedge \beta) + (-1)^p \omega \wedge \d{(\eta \wedge \beta)}
\end{align*}
Proving the induction step. Therefore it suffices to prove the claim for $q = 1$ i.e. that for any $p$-from $\omega$ and any $1$-form $\eta$ that,
\begin{align*}
\d{(\omega \wedge \eta)} = \d{\omega} \wedge \eta + (-1)^p \omega \wedge \d{\eta} 
\end{align*}
To show this,consider
\begin{align*}
(\d{(\omega \wedge \eta)})_{\mu_1 \dots \mu_{p+2}} = (p + 1)^2 \partial_{[\mu_1} (\omega_{[\mu_2 \dots \mu_{p+1}} \eta_{\mu_{p+2}]]})   
\end{align*}
Because $\omega$ is totally antisymmetric,
\[ (p + 1) \omega_{[\mu_2 \dots \mu_{p+1}} \eta_{\mu_{p+2}]} = \sum_i (-1)^{p + 2 - i} \omega_{\mu_2 \dots \mu_{p + 2}} \eta_{\mu_i} \]
and therefore,
\begin{align*}
(\d{(\omega \wedge \eta)})_{\mu_1 \dots \mu_{p+2}} & = (p + 1) \sum_i (-1)^{p + 2 - i} \partial_{[\mu_1}  \omega_{\mu_2 \dots \mu_{p + 2}} \eta_{\mu_i]} = (p + 1) \sum_i (-1)^{p + 2 - i} \partial_{[\mu_1}  \omega_{\mu_2 \dots \mu_{p + 2}} \eta_{\mu_i]}
\\
& = \sum_{i,j} (-1)^{p + 2 - i} (-1)^{j - 1} \partial_{\mu_j} (\omega_{\mu_2 \cdots \mu_{p+2}} \eta_{\mu_i}) 
\\
& = \sum_{i,j} (-1)^{p + 2 - i} (-1)^{j - 1} (\partial_{\mu_j} \omega_{\mu_2 \cdots \mu_{p+2}} \eta_{\mu_i} +  \omega_{\mu_2 \cdots \mu_{p+2}} \partial_{\mu_j} \eta_{\mu_i})
\\
& = \d{\omega} \wedge \eta + (-1)^p \omega \wedge \d{\eta}
\end{align*}
where the factor of $(-1)^p$ comes from properly reordering the indices by swapping the index of the derivative operator through the $p$ indices of $\omega$. 
\end{document}